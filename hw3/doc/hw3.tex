\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{verbatim}
\usepackage{array}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hmargin=3cm,vmargin=5.0cm]{geometry}
\usepackage{epstopdf}
\graphicspath{ {./images/} }
\usepackage{caption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{multirow}
\topmargin=-1.8cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\newcommand{\HRule}{\rule{\linewidth}{1mm}}
\newcommand{\kutu}[2]{\framebox[#1mm]{\rule[-2mm]{0mm}{#2mm}}}
\newcommand{\gap}{ \\[1mm] }
\newcommand{\Q}{\raisebox{1.7pt}{$\scriptstyle\bigcirc$}}
\newcommand{\minus}{\scalebox{0.35}[1.0]{$-$}}



\lstset{
    %backgroundcolor=\color{lbcolor},
    tabsize=2,
    language=MATLAB,
    basicstyle=\footnotesize,
    numberstyle=\footnotesize,
    aboveskip={0.0\baselineskip},
    belowskip={0.0\baselineskip},
    columns=fixed,
    showstringspaces=false,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    %frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}


\begin{document}

\noindent
\HRule %\\[3mm]
\small
\begin{center}
	\LARGE \textbf{CENG 483} \\[4mm]
	\Large Introduction to Computer Vision \\[4mm]
	\normalsize Spring 2018-2019 \\
	\Large Take Home Exam 3 \\
	\Large Image Colorization \\
    \Large Student Random ID: 70 \\
\end{center}
\HRule

\begin{center}
\end{center}
\vspace{-10mm}
\noindent\\ \\ 
Please fill in the sections below only with the requested information. If you have additional things to mention, you can use the last section. Please note that all of the results in this report should be given for the \textbf{validation set} by default, unless otherwise specified. Also, when you are expected to comment on the effect of a parameter, please make sure to \textbf{fix} other parameters. You may support your comments with visuals (i.e. loss plot).

\section{Baseline Architecture (30 pts)}
    Based on your qualitative results (do not forget to give them),
    \begin{itemize}
        \item Discuss effect of the number of conv layers:
        
		I have fixed the hyperparameters other than number of convolution layers such that kernel size is \textbf{3} except the last convolution layer, number of kernels is \textbf{2} and learning rate is \textbf{0.1}.
		
		\begin{minipage}{\textwidth}
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
   						\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(5,0.010491) (10,0.009884) (15,0.009591) (20,0.009465) (25,0.009364) (30,0.009319) (35,0.009298) (40,0.009286) (45,0.009300) (50,0.009291) (55,0.009279) (60,0.009286) (65,0.009282) (70,0.009291) (75,0.009255) (80,0.009272) (85,0.009263) (90,0.009255) (95,0.009268) (100,0.009264)};
				        \addplot+[smooth] coordinates
				        {(5,0.011484) (10,0.010445) (15,0.009895) (20,0.009393) (25,0.009234) (30,0.009068) (35,0.009049) (40,0.009090) (45,0.008953) (50,0.008902) (55,0.008872) (60,0.008849) (65,0.008896) (70,0.008793) (75,0.008827) (80,0.008764) (85,0.008739) (90,0.008753) (95,0.008759) (100,0.008698)};
				        \addplot+[smooth] coordinates
				        {(5,0.013766) (10,0.013496) (15,0.010761) (20,0.011226) (25,0.010108) (30,0.023042) (35,0.009738) (40,0.009552) (45,0.009766) (50,0.009729) (55,0.010536) (60,0.009586) (65,0.010982) (70,0.010232) (75,0.010641) (80,0.012068) (85,0.009587) (90,0.009449) (95,0.009645) (100,0.009527)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Conv Layers}};
   						\addlegendentry{1}
   						\addlegendentry{2}
   						\addlegendentry{4}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=.9\textwidth}
				\captionof{figure}{Validation loss of convolutional neural networks with different number of convolutional layers in the network over 100 epoch}
			 \end{minipage}
			 \hfill
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tabular}{ | c | c | }
				  \hline			
				  \bf Convolutional Layers & \bf Validation Loss \\
				  \hline		
				  1 & 0.009255 \\
				  \hline	
				  2 & 0.008698 \\
				  \hline	
				  4 & 0.009449 \\
				  \hline
				\end{tabular}
				\captionsetup{width=.8\textwidth}
				\captionof{table}{The lowest validation losses of convolutional neural networks achieved with different number of convolutional layers in the network}
			\end{minipage}
		\end{minipage} \\
		
		For the effect of the number of convolutional layers, the training process has achieved minimum validation loss with \textbf{2 convolutional layers} where kernel size is 3, number of kernels is 2 and learning rate 0.1 when maximum number of epoch is 100. \\
		Minimum validation loss, 0.008698, is achieved in $100^{th}$ epoch. There is a chance that model would be better if I had trained it more. Architecture with 2 convolutional layers seems better than having 1 or 4 layers for the fixed hyperparameters with the given dataset. I do think that 4 convolutional layers might be better if the model is trained with more epochs. The reason that 4 layers has achieved worse result than 2 layers might be insufficient epochs as it has more weights to optimize.
		        
        \item Discuss effect of the kernel size(except the last conv layer):
        
		I have fixed the hyperparameters other than kernel size such that the number of convolutional layer is \textbf{2}, the number of kernels is \textbf{2} and learning rate is \textbf{0.1}.
		
		\begin{minipage}{\textwidth}
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
   						\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(5,0.011484) (10,0.010445) (15,0.009895) (20,0.009393) (25,0.009234) (30,0.009068) (35,0.009049) (40,0.009090) (45,0.008953) (50,0.008902) (55,0.008872) (60,0.008849) (65,0.008896) (70,0.008793) (75,0.008827) (80,0.008764) (85,0.008739) (90,0.008753) (95,0.008759) (100,0.008698)};
				        \addplot+[smooth] coordinates
				        {(5,0.010234) (10,0.009551) (15,0.009359) (20,0.009268) (25,0.009102) (30,0.009012) (35,0.008987) (40,0.008861) (45,0.008804) (50,0.008840) (55,0.008730) (60,0.008744) (65,0.008696) (70,0.008734) (75,0.008661) (80,0.008637) (85,0.008629) (90,0.008628) (95,0.008643) (100,0.008588)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Kernel Size}};
   						\addlegendentry{3}
   						\addlegendentry{5}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=.9\textwidth}
				\captionof{figure}{Validation loss of convolutional neural network with different kernel sizes over 100 epoch}
			 \end{minipage}
			 \hfill
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tabular}{ | c | c | }
				  \hline			
				  \bf Kernel Size & \bf Validation Loss \\
				  \hline		
				  3 & 0.008698 \\
				  \hline	
				  5 & 0.008588 \\
				  \hline
				\end{tabular}
				\captionsetup{width=.8\textwidth}
				\captionof{table}{The lowest validation losses of convolutional neural networks achieved with different kernel sizes}
			\end{minipage}
		\end{minipage} \\
		
		For the effect of the number of the kernel size, the training process has achieved minimum validation loss with \textbf{kernel size 5} where number of conv layers is 2, number of kernels is 2 and learning rate 0.1 when maximum number of epoch is 100. \\
		Minimum validation loss, 0.008588, is achieved in $100^{th}$ epoch. Again, there is a chance that model would achieve lesser validation loss if I had trained it more. Having 5x5 kernels seems better than having 3x3 kernels for the fixed hyperparameters with the given dataset. Lesser validation loss might be achieved with the combination of different kernel sizes.
        
        \item Discuss effect of the number of kernels(except the last conv layer):
        
        I have fixed the hyperparameters other than the number of kernels (except the last convolutional layer) such that the number of convolutional layer is \textbf{2}, kernel size is \textbf{5} and learning rate is \textbf{0.1}.
        
		\begin{minipage}{\textwidth}
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
   						\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(5,0.010234) (10,0.009551) (15,0.009359) (20,0.009268) (25,0.009102) (30,0.009012) (35,0.008987) (40,0.008861) (45,0.008804) (50,0.008840) (55,0.008730) (60,0.008744) (65,0.008696) (70,0.008734) (75,0.008661) (80,0.008637) (85,0.008629) (90,0.008628) (95,0.008643) (100,0.008588)};
				        \addplot+[smooth] coordinates
				        {(5,0.010351) (10,0.009389) (15,0.009033) (20,0.008937) (25,0.008731) (30,0.008654) (35,0.008645) (40,0.008651) (45,0.008646) (50,0.008512) (55,0.008462) (60,0.008535) (65,0.008449) (70,0.008464) (75,0.008396) (80,0.008602) (85,0.008385) (90,0.008640) (95,0.008545) (100,0.008538)};
				        \addplot+[smooth] coordinates
				        {(5,0.010307) (10,0.009285) (15,0.009076) (20,0.008785) (25,0.008555) (30,0.008500) (35,0.008565) (40,0.008418) (45,0.008379) (50,0.008600) (55,0.008434) (60,0.008354) (65,0.008312) (70,0.008322) (75,0.008349) (80,0.008267) (85,0.008458) (90,0.008434) (95,0.008313) (100,0.008312)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Number of Kernels}};
   						\addlegendentry{2}
   						\addlegendentry{4}
   						\addlegendentry{8}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=.9\textwidth}
				\captionof{figure}{Validation loss of convolutional neural networks with different number of kernels over 100 epoch}
			 \end{minipage}
			 \hfill
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tabular}{ | c | c | }
				  \hline			
				  \bf The Number of Kernels & \bf Validation Loss \\
				  \hline		
				  2 & 0.008588 \\
				  \hline	
				  4 & 0.008385 \\
				  \hline	
				  8 & 0.008267 \\
				  \hline
				\end{tabular}
				\captionsetup{width=.8\textwidth}
				\captionof{table}{The lowest validation losses of convolutional neural networks achieved with different number of kernels}
			\end{minipage}
		\end{minipage} \\
		
		For the effect of the number of kernels, the training process has achieved minimum validation loss with \textbf{8 channel kernels} where number of conv layers is 2, kernel size is 5 and learning rate 0.1 when maximum number of epoch is 100. \\
		Minimum validation loss, 0.008267, is achieved in $80^{th}$ epoch. There is a slight chance that model would achieve lesser validation loss if I had trained it more. Having more channel makes the model better for this task because the model extends its capabilities in colorization task.
        
        \item Discuss effect of the learning rate by choosing three values: a very large one, a very small one and a value of your choice:
        
        I have fixed the hyperparameters other than the learning rate such that the number of convolutional layer is \textbf{2}, kernel size is \textbf{5} and the number of kernels is \textbf{8}.

		\begin{minipage}{\textwidth}
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
   						\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(5,0.010307) (10,0.009285) (15,0.009076) (20,0.008785) (25,0.008555) (30,0.008500) (35,0.008565) (40,0.008418) (45,0.008379) (50,0.008600) (55,0.008434) (60,0.008354) (65,0.008312) (70,0.008322) (75,0.008349) (80,0.008267) (85,0.008458) (90,0.008434) (95,0.008313) (100,0.008312)};
				        \addplot+[smooth] coordinates
				        {(5,0.013659) (10,0.012425) (15,0.011799) (20,0.011369) (25,0.011075) (30,0.010833) (35,0.010627) (40,0.010452) (45,0.010283) (50,0.010142) (55,0.010016) (60,0.009923) (65,0.009792) (70,0.009722) (75,0.009640) (80,0.009557) (85,0.009482) (90,0.009408) (95,0.009357) (100,0.009342)};
				        \addplot+[smooth] coordinates
				        {(5,0.023399) (10,0.019873) (15,0.017698) (20,0.016331) (25,0.015445) (30,0.014838) (35,0.014389) (40,0.014032) (45,0.013732) (50,0.013470) (55,0.013235) (60,0.013022) (65,0.012832) (70,0.012658) (75,0.012501) (80,0.012356) (85,0.012223) (90,0.012100) (95,0.011987) (100,0.011880)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Learning Rate}};
   						\addlegendentry{0.1}
   						\addlegendentry{0.01}
   						\addlegendentry{0.001}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=.9\textwidth}
				\captionof{figure}{Validation loss of convolutional neural networks with different learning rates over 100 epoch}
			 \end{minipage}
			 \hfill
			\begin{minipage}{0.49\textwidth}
				\centering
				\begin{tabular}{ | c | c | }
				  \hline			
				  \bf Learning Rate & \bf Validation Loss \\
				  \hline		
				  0.1 & 0.008267 \\
				  \hline	
				  0.01 & 0.009342 \\
				  \hline	
				  0.001 & 0.011880 \\
				  \hline
				\end{tabular}
				\captionsetup{width=.8\textwidth}
				\captionof{table}{The lowest validation losses of convolutional neural networks achieved with different learning rates}
			\end{minipage}
		\end{minipage} \\
		
		For the effect of the learning rate, the training process has achieved minimum validation loss with \textbf{learning rate 0.1} where number of conv layers is 2, kernel size is 5 and number of kernels is 8 when maximum number of epoch is 100. \\
		Minimum validation loss, 0.008267, is achieved in $80^{th}$ epoch. Again, there is a slight chance that model would achieve lesser validation loss if I had trained it more. Learning rate is related with maximum number of epochs because we can achieve similar results if a model with smaller learning rate is trained with bigger maximum number of epochs. However, bigger learning rate is not always good; for example, if a model with learning rate 0.9 or 1 is trained, it doesn't learn anything as its learning loss and validation loss is \textbf{NaN}. \\
		From the validation losses above, it can be concluded that learning rate is crucial for training duration. Small learning rate extends the training duration as it takes much more time to optimize the weights to their desired values. Big learning rate makes the model untrainable as it prevents the model from optimization.

    \end{itemize}


\section{Further Experiments (20 pts)}
    I will do further experiments on the best models I have trained so far:
    
    \begin{enumerate}
        \item[\textbf{I.}] Conv Layers: \textbf{2}, Kernel Size: \textbf{5}, Kernels: \textbf{8}, Learning Rate: \textbf{0.1} $\Rightarrow$ Validation Loss: \textbf{0.008267}
        \item[\textbf{II.}] Conv Layers: \textbf{2}, Kernel Size: \textbf{3}, Kernels: \textbf{8}, Learning Rate: \textbf{0.1} $\Rightarrow$ Validation Loss: \textbf{0.008289}
        \item[\textbf{III.}] Conv Layers: \textbf{4}, Kernel Size: \textbf{5}, Kernels: \textbf{8}, Learning Rate: \textbf{0.1} $\Rightarrow$ Validation Loss: \textbf{0.008308}
        \item[\textbf{IV.}] Conv Layers: \textbf{2}, Kernel Size: \textbf{5}, Kernels: \textbf{4}, Learning Rate: \textbf{0.1} $\Rightarrow$ Validation Loss: \textbf{0.008385}
    \end{enumerate}
    
    \begin{itemize}
        \item Try adding a batch-norm layer (torch.nn.BatchNorm2d) into each convolutional layer. How does it affect the results, and, why? Keep it if it is beneficial. 

		\begin{minipage}{.95\textwidth}
			\begin{minipage}{.45\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Model
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,xtick=data
				        ,xticklabels={I, II, III, IV}
				        ,legend style={at={(0.98, 0.50)},anchor=east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
	   					\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(0,0.008267) (1,0.008289) (2,0.008308) (3,0.008385)};
				        \addplot+[smooth] coordinates
				        {(0,0.012251) (1,0.012450) (2,0.012366) (3,0.012373)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Models}};
	   					\addlegendentry{without batch-norm layer}
	   					\addlegendentry{with batch-norm layer}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=1.\textwidth}
				\captionof{figure}{Comparison of the best models with and without batch normalization layer after every convolutional layer in terms of validation loss}
			 \end{minipage}
			 \hfill
			\begin{minipage}{0.45\textwidth}
				I have added batch-norm layer after ReLU activation function. I've also tested adding batch-norm layer before ReLU activation function. Applying batch-norm layer after ReLU has achieved validation loss 0.012251 whereas applying it before ReLU has achieved validation loss 0.012328. \\
		 In terms of validation loss, adding batch-normalization layer seems to be unnecessary as it increases the validation loss. Our dataset is already in range $[-1,1]$. Therefore, normalization might be unnecessary for the input layer as the input is already in normalized shape; however, batch-normalization layer might be useful for the following layers. The current models tend to output broken or sharp pixels due to lack of activation layer in the last layer. Batch-normalization layer can reduce the number of those pixels. Yet, it might not be sufficient for that problem.
			\end{minipage}
		\end{minipage} \\

        \item Try adding a tanh activation function after the very last convolutional layer. How does it affect the results, and, why? Keep it if it is beneficial. 
        
		\begin{minipage}{.95\textwidth}
			\begin{minipage}{0.45\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Model
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,xtick=data
				        ,xticklabels={I, II, III, IV}
				        ,legend style={at={(0.02, 0.98)},anchor=north west}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
	   					\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(0,0.008267) (1,0.008289) (2,0.008308) (3,0.008385)};
				        \addplot+[smooth] coordinates
				        {(0,0.008483) (1,0.008448) (2,0.008514) (3,0.008578)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Models}};
	   					\addlegendentry{without tanh}
	   					\addlegendentry{with tanh}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=1.\textwidth}
				\captionof{figure}{Comparison of the best models with and without tanh activation function after the very last convolutional layer in terms of validation loss}
			\end{minipage}
			 \hfill
			\begin{minipage}{0.45\textwidth}
				It doesn't affect the models in a good way as the models' validation losses are not decreased. However, it improves the outputs of models even though it doesn't reduce the validation losses. There is nothing after the last layer, so our model technically doesn't map $[-1,1]^{2}$ to $[-1,1]^{2}$ because the last layer has no activation function. Therefore, current models do $[-1,1]^{2}\rightarrow[-\infty,\infty]^{2}$. Due to lack of activation function after the last layer, the models' outputs have broken or sharp pixels. tanh activation function prevents this from happening. \\
		 I used mean square error (MSE) as loss function when training the models. It might be the reason for broken/sharp pixels in the outputs. If I used 12-margin error as loss function, broken/sharp pixel problem might be fixed.
			\end{minipage}
		\end{minipage} \\        
        
        
        \item Try setting the number of channels parameter to 16. How does it affect the results, and, why? Keep it if it is beneficial. 
        
		\begin{minipage}{.95\textwidth}
			\begin{minipage}{0.45\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Model
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,xtick=data
				        ,xticklabels={I, II, III, IV}
				        ,legend style={at={(0.02, 0.98)},anchor=north west}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
	   					\addlegendimage{empty legend}
				        \addplot+[smooth] coordinates
				        {(0,0.008267) (1,0.008289) (2,0.008308) (3,0.008385)};
				        \addplot+[smooth] coordinates
				        {(0,0.008175) (1,0.008267) (2,0.008260) (3,0.008175)};
				        \addlegendentry{\hspace{-.7cm}\textbf{Models}};
	   					\addlegendentry{not 16 channels}
	   					\addlegendentry{16 channels}
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=1.\textwidth}
				\captionof{figure}{Comparison of the best models and their 16 channel versions in terms of validation loss}
			\end{minipage}
			 \hfill
			\begin{minipage}{0.45\textwidth}
				16 channel counterpart of model I and IV is same because rest of the hyperparameters are identical. In terms of validation loss, having 16 channel kernels rather than 2, 4 or 8 channels is better because having more channels means new capabilities, more feature extraction/learning. Therefore, models with 16 channels have had lesser validation loss than their counterparts.
			\end{minipage}
		\end{minipage} \\
		
		\pagebreak

	    \item[\textbf{-}] Broken/Sharp pixel problem in outputs and above experiments:
		    \begin{figure}[!h]
				\centering
				\includegraphics[width=1.0\textwidth]{p2_comparison.png}
				\caption{Outputs from different models with same input: \textbf{(a)} Model I, \textbf{(b)} Model I with Batch-Normalization, \textbf{(c)} Model I with tanh activation fuction, \textbf{(d)} Model I with 16 channels instead of 8 channels}
			\end{figure}
        
    \end{itemize}
\section{Your Best Configuration (20 pts)}
My best configuration is \textbf{2 convolutional layers}, \textbf{kernel size 5}, \textbf{16 channels in kernels}, \textbf{learning rate 0.1} with \textbf{tanh} activation function after the last convolutional layer.
 
    \begin{itemize}
        \item The automatically chosen number of epochs(what was your strategy?):
        
		I have ended the training process if the latest validation loss is greater than the three times of the minimum validation loss. I have also saved the model with minimum validation losses. Therefore, I will have the best trained model saved even though the training process is finished due to extreme increase in validation loss. In this way, training phase is terminated when the model is overfitting. However, I haven't came upon termination of the training. Maybe, thats reason is that I do get the validation loss at every 5 epoch in order to fasten the training process.
        
        \item The plot of the training mean-squared error loss over epochs:
		 
		 \begin{minipage}{.9\textwidth}
			\begin{minipage}{.44\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Training Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
				        \addplot[smooth,blue] coordinates
				        {(1,0.015407) (2,0.010995) (3,0.010296) (4,0.009859) (5,0.009526) (6,0.009290) (7,0.009133) (8,0.009024) (9,0.008895) (10,0.008852) (11,0.008790) (12,0.008744) (13,0.008679) (14,0.008672) (15,0.008619) (16,0.008577) (17,0.008545) (18,0.008534) (19,0.008515) (20,0.008468) (21,0.008452) (22,0.008461) (23,0.008430) (24,0.008423) (25,0.008393) (26,0.008370) (27,0.008374) (28,0.008376) (29,0.008347) (30,0.008330) (31,0.008320) (32,0.008319) (33,0.008302) (34,0.008305) (35,0.008298) (36,0.008282) (37,0.008262) (38,0.008277) (39,0.008272) (40,0.008253) (41,0.008267) (42,0.008260) (43,0.008244) (44,0.008246) (45,0.008241) (46,0.008240) (47,0.008211) (48,0.008204) (49,0.008212) (50,0.008214) (51,0.008207) (52,0.008205) (53,0.008217) (54,0.008198) (55,0.008215) (56,0.008192) (57,0.008198) (58,0.008180) (59,0.008194) (60,0.008180) (61,0.008181) (62,0.008177) (63,0.008177) (64,0.008160) (65,0.008176) (66,0.008156) (67,0.008157) (68,0.008163) (69,0.008160) (70,0.008152) (71,0.008145) (72,0.008150) (73,0.008146) (74,0.008145) (75,0.008138) (76,0.008127) (77,0.008115) (78,0.008176) (79,0.008136) (80,0.008136) (81,0.008137) (82,0.008127) (83,0.008116) (84,0.008142) (85,0.008133) (86,0.008127) (87,0.008130) (88,0.008112) (89,0.008124) (90,0.008132) (91,0.008116) (92,0.008115) (93,0.008109) (94,0.008109) (95,0.008115) (96,0.008123) (97,0.008118) (98,0.008122) (99,0.008108) (100,0.008101)};
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=1.\textwidth}
				\captionof{figure}{Mean-Squared Error Loss of Training Data over 100 Epochs}
		 	\end{minipage}
			\hfill
			\begin{minipage}{.44\textwidth}
				\centering
				\begin{tikzpicture}
				    \begin{axis}
				        [
				        ,xlabel=Epoch
				        ,ylabel=Validation Loss
				        ,yticklabel style={/pgf/number format/fixed}
				        ,legend style={anchor=north east}
				        ,smooth
				        ,thick
				        ,mark=*
				        ]
				        \addplot[smooth,red] coordinates
				        {(1,0.011588) (2,0.010767) (3,0.010757) (4,0.009893) (5,0.009685) (6,0.009415) (7,0.009177) (8,0.009628) (9,0.008937) (10,0.009038) (11,0.008831) (12,0.009039) (13,0.008716) (14,0.008696) (15,0.008784) (16,0.008624) (17,0.008625) (18,0.008999) (19,0.008609) (20,0.008517) (21,0.008644) (22,0.008626) (23,0.008735) (24,0.008659) (25,0.008548) (26,0.008581) (27,0.008474) (28,0.008775) (29,0.008428) (30,0.008487) (31,0.008456) (32,0.008435) (33,0.008380) (34,0.008408) (35,0.008489) (36,0.008356) (37,0.008351) (38,0.008876) (39,0.008322) (40,0.008448) (41,0.008410) (42,0.008365) (43,0.008527) (44,0.008303) (45,0.008415) (46,0.008294) (47,0.008347) (48,0.008537) (49,0.008309) (50,0.008479) (51,0.008558) (52,0.008381) (53,0.008313) (54,0.008298) (55,0.008309) (56,0.008309) (57,0.008359) (58,0.008354) (59,0.008278) (60,0.008338) (61,0.008485) (62,0.008435) (63,0.008244) (64,0.008259) (65,0.008500) (66,0.008263) (67,0.008330) (68,0.008274) (69,0.008314) (70,0.008281) (71,0.008233) (72,0.008309) (73,0.008247) (74,0.008236) (75,0.008315) (76,0.008318) (77,0.008214) (78,0.008442) (79,0.008261) (80,0.008254) (81,0.008275) (82,0.008231) (83,0.008310) (84,0.008245) (85,0.008274) (86,0.008241) (87,0.008246) (88,0.008315) (89,0.008327) (90,0.008202) (91,0.008264) (92,0.008282) (93,0.008294) (94,0.008466) (95,0.008201) (96,0.008226) (97,0.008226) (98,0.008212) (99,0.008220) (100,0.008215)};
				    \end{axis}
				\end{tikzpicture}
				\captionsetup{width=1.\textwidth}
				\captionof{figure}{Mean-Squared Error Loss of Validation Data over 100 Epochs}
		 	\end{minipage}
		\end{minipage} \\
        
        \item The  plot  of  the  validation  12-margin  error  over  epochs:
        
        \begin{minipage}{1.\textwidth}
			\centering
			\begin{tikzpicture}
			    \begin{axis}
			        [
			        ,xlabel=Epoch
			        ,ylabel=12-Margin Error
			        ,yticklabel style={/pgf/number format/fixed}
			        ,legend style={anchor=north east}
			        ,smooth
			        ,thick
			        ,mark=*
			        ]
			        \addplot[smooth,orange] coordinates
			        {(1,0.273652) (2,0.272719) (3,0.273894) (4,0.249932) (5,0.285110) (6,0.247935) (7,0.271892) (8,0.283263) (9,0.256832) (10,0.249972) (11,0.267790) (12,0.253643) (13,0.263379) (14,0.244518) (15,0.255030) (16,0.252923) (17,0.263923) (18,0.229820) (19,0.258418) (20,0.250687) (21,0.262334) (22,0.244911) (23,0.283723) (24,0.255697) (25,0.257704) (26,0.254780) (27,0.248221) (28,0.254862) (29,0.243384) (30,0.264318) (31,0.256902) (32,0.238257) (33,0.256891) (34,0.236506) (35,0.266472) (36,0.250194) (37,0.251643) (38,0.292099) (39,0.244972) (40,0.266735) (41,0.264380) (42,0.245134) (43,0.251377) (44,0.252298) (45,0.258970) (46,0.236016) (47,0.254156) (48,0.245989) (49,0.250858) (50,0.261913) (51,0.211376) (52,0.259450) (53,0.265949) (54,0.232092) (55,0.252308) (56,0.242696) (57,0.269305) (58,0.266183) (59,0.253742) (60,0.267996) (61,0.255044) (62,0.252838) (63,0.249472) (64,0.235846) (65,0.261650) (66,0.246094) (67,0.245168) (68,0.257642) (69,0.265989) (70,0.258717) (71,0.236571) (72,0.248392) (73,0.237851) (74,0.245698) (75,0.250153) (76,0.224446) (77,0.236676) (78,0.247107) (79,0.255228) (80,0.244897) (81,0.249303) (82,0.250128) (83,0.239479) (84,0.221432) (85,0.265357) (86,0.245592) (87,0.243502) (88,0.266343) (89,0.259149) (90,0.239476) (91,0.257818) (92,0.242200) (93,0.262986) (94,0.268246) (95,0.245754) (96,0.235371) (97,0.234612) (98,0.245678) (99,0.259695) (100,0.230559)};
			    \end{axis}
			\end{tikzpicture}
			\captionsetup{width=.9\textwidth}
			\captionof{figure}{12-Margin Error Loss of Validation Data over 100 Epochs}
	 	\end{minipage}
	 	
	 	\pagebreak
        
        \item At least 5 qualitative results on the validation set, showing the prediction and the target colored image:
        
        \begin{figure}[!h]
			\centering
			\includegraphics[width=.7\textwidth]{p3_results.png}
			\caption{6 qualitative results from the validation set}
		\end{figure}
        
	 	\pagebreak
	 	
        \item Discuss the advantages and disadvantages of the model, based on your qualitative results, and, briefly discuss potential ways to improve the model:
        
		The model currently achieves \textbf{0.008201 validation loss} and \textbf{0.211376 12-margin error} in the validation set. The model has still sharp pixels as it can be seen from the outputs in \textit{\textbf{Figure 12}}. Those are caused from the output of last convolutional layer whose value is greater than 1 or smaller than -1. It can be fixed with simple value cropping or tanh activation function after the last convolutional layer. However, the latter solution isn't preferable as it decreases the loss and 12-margin error values. \\
		The model can be improved even more with increasing the number of kernel channels, better optimizers such as \textit{adam} or optimizing the parameters of SGD optimizer. Introducing new layers might also improve the model if it is used with combinations of different sized kernels. However, adding new layers hasn't improved the model in this homework. \\
		The model can also be trained with more epochs and suitable learning rate with proper decay rate.
        
    \end{itemize}
    
\section{Your Results on the Test Set(30 pts)}
This part will be obtained by us using the estimations you will provide. Please tell us how should we run your code in case of a problem:

\section{Additional Comments and References}

    (if there any)



\end{document}


